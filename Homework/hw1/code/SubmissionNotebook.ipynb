{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9cfbee11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up\n",
    "import json\n",
    "import collections\n",
    "import argparse\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from util import *\n",
    "\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b6b415f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem 2.2\n",
    "def extract_unigram_features(ex):\n",
    "    \"\"\"Return unigrams in the hypothesis and the premise.\n",
    "    Parameters:\n",
    "        ex : dict\n",
    "            Keys are gold_label (int, optional), sentence1 (list), and sentence2 (list)\n",
    "    Returns:\n",
    "        A dictionary of BoW featurs of x.\n",
    "    Example:\n",
    "        \"I love it\", \"I hate it\" --> {\"I\":2, \"it\":2, \"hate\":1, \"love\":1}\n",
    "    \"\"\"\n",
    "    # BEGIN_YOUR_CODE\n",
    "    # combine sentences\n",
    "    combo_sent = ex['sentence1'] + ex['sentence2']\n",
    "\n",
    "    # count words\n",
    "    bow = collections.Counter(combo_sent)\n",
    "    \n",
    "    return dict(bow)\n",
    "    # END_YOUR_CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "64f9b6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem 2.4\n",
    "def learn_predictor(train_data, valid_data, feature_extractor, learning_rate, num_epochs):\n",
    "    \"\"\"Running SGD on training examples using the logistic loss.\n",
    "    You may want to evaluate the error on training and dev example after each epoch.\n",
    "    Take a look at the functions predict and evaluate_predictor in util.py,\n",
    "    which will be useful for your implementation.\n",
    "    Parameters:\n",
    "        train_data : [{gold_label: {0,1}, sentence1: [str], sentence2: [str]}]\n",
    "        valid_data : same as train_data\n",
    "        feature_extractor : function\n",
    "            data (dict) --> feature vector (dict)\n",
    "        learning_rate : float\n",
    "        num_epochs : int\n",
    "    Returns:\n",
    "        weights : dict\n",
    "            feature name (str) : weight (float)\n",
    "    \"\"\"\n",
    "    # BEGIN_YOUR_CODE\n",
    "    # initialize weight dict\n",
    "    weights = {}\n",
    "    \n",
    "    for epoch in range(num_epochs): \n",
    "        for ex in train_data:\n",
    "            # get BoW for each ex\n",
    "            bow = feature_extractor(ex)\n",
    "            \n",
    "            # get label\n",
    "            y = ex['gold_label']\n",
    "            \n",
    "            # predict prob with current weights\n",
    "            y_pred = predict(weights, bow)\n",
    "            \n",
    "            # update weights\n",
    "            for f, v in bow.items():\n",
    "                # calc gradient\n",
    "                gradient = (y_pred - y) * v #f_w(x^{(i)})-y^{(i)}*Ï•(x^{(i)})\n",
    "                # update\n",
    "                weights[f] = weights.get(f, 0) - learning_rate * gradient\n",
    "            \n",
    "        # calc training & validation error\n",
    "        train_pred = lambda ex: 1 if predict(weights, feature_extractor(ex)) > 0.5 else 0\n",
    "        train_error = evaluate_predictor([(ex, ex['gold_label']) for ex in train_data], train_pred)\n",
    "        valid_error = evaluate_predictor([(ex, ex['gold_label']) for ex in valid_data], train_pred)\n",
    "        print(f'Epoch {epoch + 1}: train error = {train_error:.3f}, valid error = {valid_error:.3f}')\n",
    "            \n",
    "    return weights\n",
    "    # END_YOUR_CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3b4c84a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load 10000 examples from data/train.json\n",
      "Load 10000 examples from data/dev.json\n",
      "Epoch 1: train error = 0.352, valid error = 0.370\n",
      "Epoch 2: train error = 0.324, valid error = 0.364\n",
      "Epoch 3: train error = 0.306, valid error = 0.361\n",
      "Epoch 4: train error = 0.294, valid error = 0.361\n",
      "Epoch 5: train error = 0.283, valid error = 0.361\n",
      "Epoch 6: train error = 0.272, valid error = 0.362\n",
      "Epoch 7: train error = 0.267, valid error = 0.360\n",
      "Epoch 8: train error = 0.262, valid error = 0.359\n",
      "Epoch 9: train error = 0.256, valid error = 0.359\n",
      "Epoch 10: train error = 0.251, valid error = 0.360\n",
      "train error=0.251, valid error=0.36\n"
     ]
    }
   ],
   "source": [
    "def test_unigram():\n",
    "    train_data = read_dataset('data/train.json', -1)\n",
    "    valid_data = read_dataset('data/dev.json', -1)\n",
    "    feature_extractor = extract_unigram_features\n",
    "    weights = learn_predictor(train_data, valid_data, feature_extractor, 0.01, 10)\n",
    "    predictor = lambda ex: 1 if dot(weights, feature_extractor(ex)) > 0 else 0\n",
    "    train_err = evaluate_predictor([(ex, ex['gold_label']) for ex in train_data], predictor)\n",
    "    valid_err = evaluate_predictor([(ex, ex['gold_label']) for ex in valid_data], predictor)\n",
    "    print('train error={}, valid error={}'.format(train_err, valid_err))\n",
    "    error_analysis(valid_data[:100], feature_extractor, weights, 'error_analysis_unigram.txt')\n",
    "\n",
    "test_unigram()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a9c6ecbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem 2.6\n",
    "def extract_custom_features(ex):\n",
    "    \"\"\"Design your own features.\n",
    "    \"\"\"\n",
    "    # BEGIN_YOUR_CODE\n",
    "    # initialize feature dict (bow in unigram)\n",
    "    features = {}\n",
    "    \n",
    "    sentence1 = ex['sentence1']\n",
    "    sentence2 = ex['sentence2']\n",
    "    \n",
    "    # unigram with all lowercase\n",
    "    all_words = sentence1 + sentence2\n",
    "    for word in set(all_words):\n",
    "        features[f'unigram_{word.lower()}'] = all_words.count(word)\n",
    "    \n",
    "    # bigram\n",
    "    def get_bigrams(sentence):\n",
    "        return [f\"{sentence[i]}_{sentence[i+1]}\" for i in range(len(sentence)-1)]\n",
    "    \n",
    "    bigrams1 = get_bigrams(sentence1)\n",
    "    bigrams2 = get_bigrams(sentence2)\n",
    "    all_bigrams = bigrams1 + bigrams2\n",
    "    for bigram in set(all_bigrams):\n",
    "        features[f'bigram_{bigram.lower()}'] = all_bigrams.count(bigram)\n",
    "    \n",
    "    # negation\n",
    "    negation_words = set(['not', 'no', 'never', 'neither', 'nor', 'none'])\n",
    "    features['negation_s1'] = sum(1 for word in sentence1 if word.lower() in negation_words)\n",
    "    features['negation_s2'] = sum(1 for word in sentence2 if word.lower() in negation_words)\n",
    "    \n",
    "    return features\n",
    "    # END_YOUR_CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a9dd3824",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load 10000 examples from data/train.json\n",
      "Load 10000 examples from data/dev.json\n",
      "Epoch 1: train error = 0.297, valid error = 0.367\n",
      "Epoch 2: train error = 0.243, valid error = 0.358\n",
      "Epoch 3: train error = 0.210, valid error = 0.353\n",
      "Epoch 4: train error = 0.188, valid error = 0.351\n",
      "Epoch 5: train error = 0.169, valid error = 0.349\n",
      "Epoch 6: train error = 0.151, valid error = 0.350\n",
      "Epoch 7: train error = 0.138, valid error = 0.350\n",
      "Epoch 8: train error = 0.127, valid error = 0.350\n",
      "Epoch 9: train error = 0.115, valid error = 0.350\n",
      "Epoch 10: train error = 0.105, valid error = 0.351\n",
      "train error=0.1055, valid error=0.3514\n"
     ]
    }
   ],
   "source": [
    "## performs better on validation (dev) than unigram\n",
    "def test_custom():\n",
    "    train_data = read_dataset('data/train.json', -1)\n",
    "    valid_data = read_dataset('data/dev.json', -1)\n",
    "    feature_extractor = extract_custom_features\n",
    "    weights = learn_predictor(train_data, valid_data, feature_extractor, 0.01, 10)\n",
    "    predictor = lambda ex: 1 if dot(weights, feature_extractor(ex)) > 0 else 0\n",
    "    train_err = evaluate_predictor([(ex, ex['gold_label']) for ex in train_data], predictor)\n",
    "    valid_err = evaluate_predictor([(ex, ex['gold_label']) for ex in valid_data], predictor)\n",
    "    print('train error={}, valid error={}'.format(train_err, valid_err))\n",
    "    error_analysis(valid_data[:100], feature_extractor, weights, 'error_analysis_custom.txt')\n",
    "    \n",
    "test_custom()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ae4ca3a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load 10000 examples from data/train.json\n",
      "Load 10000 examples from data/dev.json\n",
      "Epoch 1: train error = 0.318, valid error = 0.315\n",
      "Epoch 2: train error = 0.298, valid error = 0.301\n",
      "Epoch 3: train error = 0.288, valid error = 0.295\n",
      "Epoch 4: train error = 0.280, valid error = 0.295\n",
      "Epoch 5: train error = 0.273, valid error = 0.292\n",
      "Epoch 6: train error = 0.266, valid error = 0.289\n",
      "Epoch 7: train error = 0.260, valid error = 0.287\n",
      "Epoch 8: train error = 0.255, valid error = 0.285\n",
      "Epoch 9: train error = 0.251, valid error = 0.282\n",
      "Epoch 10: train error = 0.247, valid error = 0.281\n",
      "train error=0.2468, valid error=0.2805\n"
     ]
    }
   ],
   "source": [
    "# Problem 2.8\n",
    "def extract_unigram_features(ex):\n",
    "    \"\"\"Return unigrams in the hypothesis and the premise.\n",
    "    Parameters:\n",
    "        ex : dict\n",
    "            Keys are gold_label (int, optional), sentence1 (list), and sentence2 (list)\n",
    "    Returns:\n",
    "        A dictionary of BoW featurs of x.\n",
    "    Example:\n",
    "        \"I love it\", \"I hate it\" --> {\"I\":2, \"it\":2, \"hate\":1, \"love\":1}\n",
    "    \"\"\"\n",
    "    # BEGIN_YOUR_CODE\n",
    "    # combine sentences\n",
    "    sent2 = ex['sentence2']\n",
    "\n",
    "    # count words\n",
    "    bow = collections.Counter(sent2)\n",
    "    \n",
    "    return dict(bow)\n",
    "    # END_YOUR_CODE\n",
    "    \n",
    "    \n",
    "def test_unigram():\n",
    "    train_data = read_dataset('data/train.json', -1)\n",
    "    valid_data = read_dataset('data/dev.json', -1)\n",
    "    feature_extractor = extract_unigram_features\n",
    "    weights = learn_predictor(train_data, valid_data, feature_extractor, 0.01, 10)\n",
    "    predictor = lambda ex: 1 if dot(weights, feature_extractor(ex)) > 0 else 0\n",
    "    train_err = evaluate_predictor([(ex, ex['gold_label']) for ex in train_data], predictor)\n",
    "    valid_err = evaluate_predictor([(ex, ex['gold_label']) for ex in valid_data], predictor)\n",
    "    print('train error={}, valid error={}'.format(train_err, valid_err))\n",
    "    error_analysis(valid_data[:100], feature_extractor, weights, 'error_analysis_unigram.txt')\n",
    "\n",
    "test_unigram()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55883965",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem 3.1\n",
    "def count_cooccur_matrix(tokens, window_size=4):\n",
    "    \"\"\"Compute the co-occurrence matrix given a sequence of tokens.\n",
    "    For each word, n words before and n words after it are its co-occurring neighbors.\n",
    "    For example, given the tokens \"in for a penny , in for a pound\",\n",
    "    the neighbors of \"penny\" given a window size of 2 are \"for\", \"a\", \",\", \"in\".\n",
    "    Parameters:\n",
    "        tokens : [str]\n",
    "        window_size : int\n",
    "    Returns:\n",
    "        word2ind : dict\n",
    "            word (str) : index (int)\n",
    "        co_mat : np.array\n",
    "            co_mat[i][j] should contain the co-occurrence counts of the words indexed by i \n",
    "            and j according to the dictionary word2ind.\n",
    "    \"\"\"\n",
    "    # BEGIN_YOUR_CODE\n",
    "    # dict for words & corresponding indices (w=word, i=index)\n",
    "    word2ind = {w: i for i, w in enumerate(set(tokens))}\n",
    "    \n",
    "    # initialize co-occurence matrix\n",
    "    vocab_len = len(word2ind)\n",
    "    co_mat = np.zeros((vocab_len, vocab_len), dtype = int)\n",
    "    \n",
    "    for i, t in enumerate(tokens):\n",
    "        token_ind = word2ind[t] #gets index of current token\n",
    "        \n",
    "        # window range\n",
    "        start = max(0, i - window_size)\n",
    "        end = min(len(tokens), i + window_size + 1)\n",
    "        \n",
    "        # count co-occurrences\n",
    "        for n in range(start, end):\n",
    "            if i != n: #skip current token\n",
    "                neighbor = tokens[n]\n",
    "                neighbor_ind = word2ind[neighbor]\n",
    "                co_mat[token_ind][neighbor_ind] += 1\n",
    "                \n",
    "    return word2ind, co_mat\n",
    "    # END_YOUR_CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2fc233db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem 3.3\n",
    "def cooccur_to_embedding(co_mat, embed_size=50):\n",
    "    \"\"\"Convert the co-occurrence matrix to word embedding using truncated SVD. Use the np.linalg.svd function.\n",
    "    Parameters:\n",
    "        co_mat : np.array\n",
    "            vocab size x vocab size\n",
    "        embed_size : int\n",
    "    Returns:\n",
    "        embeddings : np.array\n",
    "            vocab_size x embed_size\n",
    "    \"\"\"\n",
    "    # BEGIN_YOUR_CODE\n",
    "    '''# apply log transformation to co_mat\n",
    "    log_co_mat = np.log(co_mat + 1) #add 1 to prevent log(0)\n",
    "    \n",
    "    # truncated SVD - truncate based on embed_size\n",
    "    U, S, Vt = np.linalg.svd(log_co_mat, full_matrices = False, hermitian = False) #set full_matrices=False since 2d vector after log transformation\n",
    "    U, S, Vt = np.linalg.svd(co_mat, hermitian = False)\n",
    "    \n",
    "    U_trunc = U[:, :embed_size]\n",
    "    S_trunc = S[:embed_size]\n",
    "\n",
    "    # calc embeddings\n",
    "    embeddings = U_trunc * np.sqrt(S_trunc)'''\n",
    "    U, s, Vt = np.linalg.svd(co_mat, full_matrices=False)\n",
    "    \n",
    "    # truncate to the specified embedding size\n",
    "    U_truncated = U[:, :embed_size]\n",
    "    s_truncated = s[:embed_size]\n",
    "    \n",
    "    # compute the embeddings\n",
    "    embeddings = -U_truncated * s_truncated[np.newaxis, :]\n",
    "            \n",
    "    return embeddings\n",
    "    # END_YOUR_CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "285e0a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem 3.4\n",
    "def top_k_similar(word_ind, embeddings, word2ind, k=10, metric='dot'):\n",
    "    \"\"\"Return the top k most similar words to the given word (excluding itself).\n",
    "    You will implement two similarity functions.\n",
    "    If metric='dot', use the dot product.\n",
    "    If metric='cosine', use the cosine similarity.\n",
    "    Parameters:\n",
    "        word_ind : int\n",
    "            index of the word (for which we will find the similar words)\n",
    "        embeddings : np.array\n",
    "            vocab_size x embed_size\n",
    "        word2ind : dict\n",
    "        k : int\n",
    "            number of words to return (excluding self)\n",
    "        metric : 'dot' or 'cosine'\n",
    "    Returns:\n",
    "        topk_words : [str]\n",
    "    \"\"\"\n",
    "    # BEGIN_YOUR_CODE\n",
    "    # get embedding of target word\n",
    "    target_embed = embeddings[word_ind]\n",
    "    \n",
    "    # compute similarities\n",
    "    if metric == 'dot':\n",
    "        similarities = np.dot(embeddings, target_embed)\n",
    "    else: #place holder for 'cosine'\n",
    "        print('Cosine similarity selected')\n",
    "        pass\n",
    "    \n",
    "    # adjust similarity btwn word & itself to be very low\n",
    "    similarities[word_ind] = float('-inf')\n",
    "    \n",
    "    # get indices of top k similar words\n",
    "    top_k_inds = np.argsort(similarities)[-k:][::-1]\n",
    "    \n",
    "    # get words associated with indices (w=word, i=index)\n",
    "    ind2word = {i: w for w, i in word2ind.items()}\n",
    "    topk_words = [ind2word[i] for i in top_k_inds]\n",
    "    \n",
    "    return topk_words\n",
    "    # END_YOUR_CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d1b10fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     /Users/sophiajuco/nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top k most similar words to man\n",
      "and i it the \" in she but that ,\n",
      "top k most similar words to woman\n",
      "and , the i it in \" of to was\n",
      "top k most similar words to happy\n",
      ", and the mr i be it to her a\n",
      "top k most similar words to sad\n",
      "of and in , . was very to is with\n",
      "top k most similar words to emma\n",
      "and i \" , it she mr he the you\n",
      "top k most similar words to knightley\n",
      "mr mrs i and it she \" weston elton he\n"
     ]
    }
   ],
   "source": [
    "# Problem 3.5\n",
    "def test_embedding(words=['man', 'woman', 'happy', 'sad', 'emma', 'knightley']):\n",
    "    tokens = read_corpus()\n",
    "    word2ind, co_mat = count_cooccur_matrix(tokens, window_size=1)\n",
    "    embeddings = cooccur_to_embedding(co_mat, 25)\n",
    "    for word in words:\n",
    "        word_ind = word2ind[word]\n",
    "        top_k_words = top_k_similar(word_ind, embeddings, word2ind, k=10, metric='dot')\n",
    "        print('top k most similar words to', word)\n",
    "        print(' '.join(top_k_words))\n",
    "    return embeddings\n",
    "\n",
    "test2 = test_embedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f45fbefd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.32778902, -0.49065546,  0.05708003, ..., -0.39414713,\n",
       "        -1.44234663, -0.39142897],\n",
       "       [-0.45656484, -0.24753939, -0.25098383, ...,  0.08277396,\n",
       "        -0.14086778,  0.3123277 ],\n",
       "       [-0.12532598,  0.05601888,  0.02379837, ...,  0.08075457,\n",
       "        -0.09662852,  0.14346088],\n",
       "       ...,\n",
       "       [-0.94835287, -1.07232428,  0.75208154, ...,  0.1094477 ,\n",
       "         0.07309543, -0.21885018],\n",
       "       [-0.32009247, -0.03828584, -0.33083983, ..., -0.01092303,\n",
       "         0.06792273,  0.65760978],\n",
       "       [-1.16046274,  0.04440773, -0.06263916, ..., -0.10973086,\n",
       "         0.17192602,  0.32812246]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "15afd55e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.32778902,  0.49065546, -0.05708003, ...,  0.39414713,\n",
       "         1.44234663,  0.39142897],\n",
       "       [ 0.45656484,  0.24753939,  0.25098383, ..., -0.08277396,\n",
       "         0.14086778, -0.3123277 ],\n",
       "       [ 0.12532598, -0.05601888, -0.02379837, ..., -0.08075457,\n",
       "         0.09662852, -0.14346088],\n",
       "       ...,\n",
       "       [ 0.94835287,  1.07232428, -0.75208154, ..., -0.1094477 ,\n",
       "        -0.07309543,  0.21885018],\n",
       "       [ 0.32009247,  0.03828584,  0.33083983, ...,  0.01092303,\n",
       "        -0.06792273, -0.65760978],\n",
       "       [ 1.16046274, -0.04440773,  0.06263916, ...,  0.10973086,\n",
       "        -0.17192602, -0.32812246]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "43e544e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1., -1., -1., -1., -1., -1.,  1.,  1.,  1., -1., -1.,  1.,  1.,\n",
       "       -1., -1., -1.,  1.,  1.,  1.,  1.,  1., -1.,  1., -1.,  1.])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sign(test1[:][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dda9343d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.04036213,  0.05763836, -0.00996007, ...,  0.00808308,\n",
       "         0.02513569,  0.01602217],\n",
       "       [-0.03936486,  0.03765437,  0.00375551, ..., -0.00895273,\n",
       "        -0.02419763, -0.06151337],\n",
       "       [-0.00661321,  0.00558734, -0.00437265, ..., -0.00838526,\n",
       "        -0.00736296,  0.00481276],\n",
       "       ...,\n",
       "       [-0.1479234 ,  0.09023045,  0.25375   , ...,  0.01438565,\n",
       "         0.04689914,  0.05125293],\n",
       "       [-0.01197331,  0.00372352,  0.0127503 , ...,  0.01344636,\n",
       "         0.02508408, -0.02837081],\n",
       "       [-0.06456793,  0.04396606,  0.01948028, ..., -0.02215805,\n",
       "        -0.0089393 , -0.00366436]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "621ac944",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem 3.6\n",
    "def top_k_similar(word_ind, embeddings, word2ind, k=10, metric='dot'):\n",
    "    \"\"\"Return the top k most similar words to the given word (excluding itself).\n",
    "    You will implement two similarity functions.\n",
    "    If metric='dot', use the dot product.\n",
    "    If metric='cosine', use the cosine similarity.\n",
    "    Parameters:\n",
    "        word_ind : int\n",
    "            index of the word (for which we will find the similar words)\n",
    "        embeddings : np.array\n",
    "            vocab_size x embed_size\n",
    "        word2ind : dict\n",
    "        k : int\n",
    "            number of words to return (excluding self)\n",
    "        metric : 'dot' or 'cosine'\n",
    "    Returns:\n",
    "        topk_words : [str]\n",
    "    \"\"\"\n",
    "    # BEGIN_YOUR_CODE\n",
    "    # get embedding of target word\n",
    "    target_embed = embeddings[word_ind]\n",
    "    \n",
    "    # compute similarities\n",
    "    if metric == 'dot':\n",
    "        similarities = np.dot(embeddings, target_embed)\n",
    "    elif metric == 'cosine':\n",
    "        norm_target = np.linalg.norm(target_embed)\n",
    "        norm_embeds = np.linalg.norm(embeddings, axis=1)\n",
    "        similarities = np.dot(embeddings, target_embed) / (norm_embeds * norm_target)\n",
    "    else: #error if neither dot or cosine\n",
    "        raise ValueError(\"Metric must be either 'dot' or 'cosine'\")\n",
    "    \n",
    "    # adjust similarity btwn word & itself to be very low\n",
    "    similarities[word_ind] = float('-inf')\n",
    "    \n",
    "    # get indices of top k similar words\n",
    "    top_k_inds = np.argsort(similarities)[-k:][::-1]\n",
    "    \n",
    "    # get words associated with indices (w=word, i=index)\n",
    "    ind2word = {i: w for w, i in word2ind.items()}\n",
    "    topk_words = [ind2word[i] for i in top_k_inds]\n",
    "    \n",
    "    return topk_words\n",
    "    # END_YOUR_CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67172c2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     /Users/sophiajuco/nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top k most similar words to man\n",
      "woman lady girl creature _man_ gentleman moment person tis farmer\n",
      "top k most similar words to woman\n",
      "man lady gentleman person _man_ tis people girl creature circumstance\n",
      "top k most similar words to happy\n",
      "obliging ready agreeable serious busy limited kind complete distressing pleasant\n",
      "top k most similar words to sad\n",
      "narrow quivering ninth lame painful delightful small lucky few striking\n",
      "top k most similar words to emma\n",
      "harriet jane mr isabella mrs frank he then knightley ,'\"\n",
      "top k most similar words to knightley\n",
      "elton weston perry churchill woodhouse martin isabella fairfax goddard dixon\n"
     ]
    }
   ],
   "source": [
    "# Problem 3.7\n",
    "def test_embedding(words=['man', 'woman', 'happy', 'sad', 'emma', 'knightley']):\n",
    "    tokens = read_corpus()\n",
    "    word2ind, co_mat = count_cooccur_matrix(tokens, window_size=1)\n",
    "    embeddings = cooccur_to_embedding(co_mat, 100)\n",
    "    for word in words:\n",
    "        word_ind = word2ind[word]\n",
    "        top_k_words = top_k_similar(word_ind, embeddings, word2ind, k=10, metric='cosine')\n",
    "        print('top k most similar words to', word)\n",
    "        print(' '.join(top_k_words))\n",
    "\n",
    "test_embedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b200d6d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
